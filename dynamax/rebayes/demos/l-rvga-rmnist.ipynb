{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52197cc5-e795-481b-abbe-8739492dfc76",
   "metadata": {},
   "source": [
    "# L-RVGA for rotation MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da73ba4-04b8-4827-b04f-406463c623e4",
   "metadata": {},
   "source": [
    "The Limited-memory Recursive Variational Gaussian Approximation equations are given by\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    q^*_t(\\theta) &= \\arg\\min_{\\mu^*, P^*}\n",
    "        \\text{KL}\\left(  \\mathcal{N}(\\theta \\vert \\mu^*, P^*)  || \\mathcal{N}(\\theta \\vert \\mu_{t-1}, P_{t-1}^{-1}) p(y_t \\vert\\theta) \\right)\\\\\n",
    "    q_t(\\theta) &= \\arg\\min_{\\mu, W, \\Psi}\n",
    "         \\text{KL}\\left( \\mathcal{N}(\\theta \\vert \\left(WW^T + \\Psi\\right)^{-1} || \\mathcal{N}(\\theta \\vert \\mu^*, P^*)   \\right)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7d10ce2-c6f8-499a-9d86-23db25e96c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "from dynamax.utils import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2030e507-5d35-4003-9d56-6c0a36561c9f",
   "metadata": {},
   "source": [
    "## Load Rotation MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b7c33d4-4de8-4179-b643-9571092a7697",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(314)\n",
    "num_train = 100\n",
    "\n",
    "train, test = datasets.load_rotated_mnist(target_digit=2)\n",
    "X_train, y_train = train\n",
    "X_test, y_test = test\n",
    "\n",
    "X_train = jnp.array(X_train)\n",
    "y_train = jnp.array(y_train)\n",
    "\n",
    "X = jnp.array(X_train)[:num_train]\n",
    "y = jnp.array(y_train)[:num_train]\n",
    "\n",
    "# ix_sort = jnp.argsort(y)\n",
    "# X = X[ix_sort]\n",
    "# y = y[ix_sort]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a83a681-2628-46a7-bbab-5070aec11fe4",
   "metadata": {},
   "source": [
    "## L-RVGA\n",
    "\n",
    "We consider a linear problem. Let $y_t = \\theta^T x_t + \\epsilon_t$, $\\epsilon_t \\sim \\mathcal{N}(0, 1)$. Considering the approximated R-VGA, the update for $\\mu_t\\in\\mathbb{R}^{d}$, $W_t\\in\\mathbb{R}^{d\\times p}$, and $\\Psi_t\\in\\mathbb{R}^{d\\times d}$ becomes\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mu_t &= \\mu_{t-1} + \\left(W_t W_t^\\intercal + \\Psi_t\\right)^{-1}x_t\\left(y_t - \\mu_{t-1}^\\intercal x_t\\right)\\\\\n",
    "W_t W_t^\\intercal + \\Psi_t &\\underset{\\text{FA}}{\\approx} \\alpha_t\\left( W_{t-1}W_{t-1}^\\intercal + \\Psi_{t-1} \\right) + \\beta x_tx_t^\\intercal\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03877f5-2454-4bb9-8260-6ed9b6ceefc4",
   "metadata": {},
   "source": [
    "The FA update for the posterior covariance is obtained by iterating `nb_inner_loop` number of times the following set of equations\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "M &= {\\bf I}_p + W^\\intercal \\Psi^{-1} W\\\\\n",
    "V &= \\beta_t x_tx_t^\\intercal\\Psi^{-1} W + \\alpha_t\\left[ W_{t-1}W_{t-1}^\\intercal \\Psi^{-1} W + \\Psi_{t-1}\\Psi^{-1} W \\right]\\\\\n",
    "W^{(n)} &= V\\left({\\bf I}_p + M^{-1} W^\\intercal\\Psi^{-1}V\\right)^{-1}\\\\\n",
    "\\Psi^{(n)} &= \\beta_t \\text{diag}\\left(x_tx_t^\\intercal\\right) + \\alpha_t \\text{diag}\\left(W_{t-1}W_{t-1}^\\intercal\\right) + \\alpha_t\\Psi_{t-1} - \\text{diag}\\left(W^{(n)}M^{-1}V^\\intercal\\right)\\\\\n",
    "W &= W^{(n)}, \\Psi = \\Psi^{(n)}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b2281b-70e9-405d-b384-aae7617ab5a6",
   "metadata": {},
   "source": [
    "### Initialisiation\n",
    "\n",
    "We consider the initialisiation rule of ยง5.1.2:\n",
    "1. $\\Psi_0 = \\psi_0 {\\bf I}_d$, where $\\psi_0 > 0$,\n",
    "2. $W_0 \\in \\mathbb{R}^{d\\times p}$,  where the columns are random vectors independently drawn from an istropic Gaussian distribution in $\\mathbb{R}^d$ and which have been normalised so that $\\forall k. ||u_k|| = w_0$.\n",
    "We let\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\psi_0 &= (1 - \\epsilon) \\frac{1}{\\sigma^2_0},\\\\\n",
    "    w_0 &= \\sqrt{\\frac{\\epsilon d}{p\\sigma^2_0}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "with $0 < \\epsilon \\ll 1$ a small parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed9386e7-0e26-4bed-a518-e5e61fffa6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X_train[0]\n",
    "num_dims, *_ = x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "97ec00f7-603e-4c0c-9f37-e864df642718",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_basis = 100\n",
    "key = jax.random.PRNGKey(314)\n",
    "\n",
    "x = jax.random.normal(key, (num_dims,))\n",
    "W = jax.random.normal(key, (num_dims, num_basis))\n",
    "Psi_inv = 1 / jax.random.normal(key, (num_dims,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "366e05e2-5330-4d23-9b78-f9c04d01fa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fa_approx():\n",
    "    num_basis = len(Psi)\n",
    "    Psi_inv = 1 / Psi\n",
    "    I = jnp.eye(num_basis)\n",
    "    M = I + jnp.einsum(\"ji,j,jk->jk\", W, inv_Psi, W)\n",
    "    M_inv = jnp.linalg.inv(M)\n",
    "    V_beta = jnp.einsum(\"i,j,j,jk->ik\", x, x, Psi_inv, W)\n",
    "    V_alpha = (\n",
    "        jnp.einsum(\"ij,kj,k,kl->il\", W_prev, W_prev, Psi_inv, W) +\n",
    "        jnp.einsum(\"i,i,ij->ij\", Psi_prev, Psi_inv, W)\n",
    "    )\n",
    "    V = beta * V_beta + alpha * V_alpha\n",
    "    # Value_update\n",
    "    W_solve = I + jnp.einsum(\"ij,kj,k,kl->il\", M_inv, W, Psi_inv, V)\n",
    "    W = jnp.linalg.solve(W_solve, V).T\n",
    "    Psi = (\n",
    "        beta * jnp.einsum(\"i,i\", x, x) +\n",
    "        alpha * jnp.einsum(\"ij,ij->i\", W_prev, W_prev) + \n",
    "        alpha * Psi_prev -\n",
    "        jnp.einsum(\"ij,jk,ik->i\", W, M_inv, V)\n",
    "    )\n",
    "    return W, Psi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
