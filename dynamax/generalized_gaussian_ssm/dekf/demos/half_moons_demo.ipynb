{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcCOwXjSjwjg"
      },
      "source": [
        "# 0. Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DpZyU1JJju0g"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    from dynamax.generalized_gaussian_ssm.inference import *\n",
        "    from dynamax.generalized_gaussian_ssm.models import ParamsGGSSM\n",
        "    import chex\n",
        "    import flax\n",
        "    import ml_collections\n",
        "except ModuleNotFoundError:\n",
        "    print('installing dynamax')\n",
        "    %pip install -qq git+https://github.com/probml/dynamax\n",
        "    print('installing chex')\n",
        "    %pip install chex\n",
        "    print('installing flax')\n",
        "    %pip install flax\n",
        "    print('installing ml_collections')\n",
        "    %pip install ml_collections\n",
        "    from dynamax.generalized_gaussian_ssm.inference import *\n",
        "    from dynamax.generalized_gaussian_ssm.models import ParamsGGSSM\n",
        "    import chex\n",
        "    import flax\n",
        "    import ml_collections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g47C1c1Vj6_i"
      },
      "outputs": [],
      "source": [
        "from typing import Sequence\n",
        "from functools import partial\n",
        "from collections import deque\n",
        "from pathlib import Path\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.random as jr\n",
        "from jax import lax, jit, vmap, pmap\n",
        "from jax.tree_util import tree_map, tree_reduce\n",
        "import flax.linen as nn\n",
        "from tensorflow_probability.substrates.jax.distributions import MultivariateNormalFullCovariance as MVN\n",
        "from tensorflow_probability.substrates.jax.distributions import MultivariateNormalDiag as MVND\n",
        "import chex\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "from jax.flatten_util import ravel_pytree\n",
        "import optax\n",
        "from tqdm import tqdm\n",
        "from tensorflow_probability.substrates import jax as tfp\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn import preprocessing\n",
        "from dynamax.generalized_gaussian_ssm.dekf.diagonal_inference import (\n",
        "    _jacrev_2d,\n",
        "    DEKFParams,\n",
        "    _stationary_dynamics_diagonal_predict,\n",
        "    _fully_decoupled_ekf_condition_on,\n",
        "    stationary_dynamics_fully_decoupled_conditional_moments_gaussian_filter,\n",
        "    _variational_diagonal_ekf_condition_on,\n",
        "    stationary_dynamics_variational_diagonal_extended_kalman_filter\n",
        ")\n",
        "from dynamax.generalized_gaussian_ssm.dekf.utils import (\n",
        "    MLP,\n",
        "    get_mlp_flattened_params,\n",
        "    loss_optax\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sm5OHLoukBNd"
      },
      "source": [
        "# 1. Dataset Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOV1Am9DPiW8"
      },
      "outputs": [],
      "source": [
        "def generate_input_grid(input):\n",
        "    \"\"\"Generate grid on input space.\n",
        "    Args:\n",
        "        input (DeviceArray): Input array to determine the range of the grid.\n",
        "    Returns:\n",
        "        input_grid: Generated input grid.\n",
        "    \"\"\"    \n",
        "    # Define grid limits\n",
        "    xmin, ymin = input.min(axis=0) - 0.1\n",
        "    xmax, ymax = input.max(axis=0) + 0.1\n",
        "\n",
        "    # Define grid\n",
        "    step = 0.1\n",
        "    x_grid, y_grid = jnp.meshgrid(jnp.mgrid[xmin:xmax:step], jnp.mgrid[ymin:ymax:step])\n",
        "    input_grid = jnp.concatenate([x_grid[...,None], y_grid[...,None]], axis=2)\n",
        "\n",
        "    return input_grid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJVl8aY1PYOp"
      },
      "outputs": [],
      "source": [
        "def posterior_predictive_grid_ekf(grid, mean, cov, apply, binary=False, ekf_type='fcekf', \n",
        "                                  post_pred_type='mc', num_samples=100,key=0):\n",
        "    \"\"\"Compute posterior predictive probability for each point in grid\n",
        "    Args:\n",
        "        grid (DeviceArray): Grid on which to predict posterior probability.\n",
        "        mean (DeviceArray): Posterior mean of parameters.\n",
        "        cov (DeviceArray): Posterior cov of parameters.\n",
        "        apply (Callable): Apply function for MLP.\n",
        "        binary (bool, optional): Flag to determine whether to round probabilities to binary outputs. Defaults to True.\n",
        "    Returns:\n",
        "        _type_: _description_\n",
        "    \"\"\"\n",
        "    if isinstance(key, int):\n",
        "        key = jr.PRNGKey(key)\n",
        "\n",
        "    if ekf_type == 'fcekf':\n",
        "        mvn = MVN(loc=mean, covariance_matrix=cov)\n",
        "    else:\n",
        "        mvn = MVND(loc=mean, scale_diag=cov)\n",
        "    # Sample parameters\n",
        "    sampled_params = mvn.sample(seed=key, sample_shape=num_samples)\n",
        "    \n",
        "    def posterior_predictive_mc(param):\n",
        "        inferred_fn = lambda x: apply(param, x)\n",
        "        fn_vec = jnp.vectorize(inferred_fn, signature='(2)->(3)')\n",
        "        Z = fn_vec(grid)\n",
        "        if binary:\n",
        "            Z = jnp.rint(Z)\n",
        "        return Z\n",
        "\n",
        "    def posterior_predictive_immer(param):\n",
        "        def inferred_fn(x):\n",
        "            apply_fn = lambda p: apply(p, x)\n",
        "            H = _jacrev_2d(apply_fn, mean)\n",
        "            return apply_fn(mean) + H @ (param - mean)\n",
        "        fn_vec = jnp.vectorize(inferred_fn, signature='(2)->(3)')\n",
        "        Z = fn_vec(grid)\n",
        "        if binary:\n",
        "            Z = jnp.rint(Z)\n",
        "        return Z\n",
        "    if post_pred_type == 'mc':\n",
        "        Zs = vmap(posterior_predictive_mc)(sampled_params)\n",
        "    else:\n",
        "        Zs = vmap(posterior_predictive_immer)(sampled_params)\n",
        "    \n",
        "    return Zs.mean(axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCAcppZ0O99Q"
      },
      "outputs": [],
      "source": [
        "def posterior_predictive_grid(grid, mean, apply, binary=False):\n",
        "    \"\"\"Compute posterior predictive probability for each point in grid\n",
        "    Args:\n",
        "        grid (DeviceArray): Grid on which to predict posterior probability.\n",
        "        mean (DeviceArray): Posterior mean of parameters.\n",
        "        apply (Callable): Apply function for MLP.\n",
        "        binary (bool, optional): Flag to determine whether to round probabilities to binary outputs. Defaults to True.\n",
        "    Returns:\n",
        "        _type_: _description_\n",
        "    \"\"\"    \n",
        "    inferred_fn = lambda x: apply(mean, x)\n",
        "    fn_vec = jnp.vectorize(inferred_fn, signature='(2)->(3)')\n",
        "    Z = fn_vec(grid)\n",
        "    if binary:\n",
        "        Z = jnp.rint(Z)\n",
        "    return Z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JE89yoYPjwZ"
      },
      "outputs": [],
      "source": [
        "def plot_posterior_predictive(ax, X, Y, title, Xspace=None, Zspace=None, cmap=cm.rainbow):\n",
        "    \"\"\"Plot the 2d posterior predictive distribution.\n",
        "    Args:\n",
        "        ax (axis): Matplotlib axis.\n",
        "        X (DeviceArray): Input array.\n",
        "        title (str): Title for the plot.\n",
        "        colors (list): List of colors that correspond to each element in X.\n",
        "        Xspace (DeviceArray, optional): Input grid to predict on. Defaults to None.\n",
        "        Zspace (DeviceArray, optional): Predicted posterior on the input grid. Defaults to None.\n",
        "        cmap (str, optional): Matplotlib colormap. Defaults to \"viridis\".\n",
        "    \"\"\"    \n",
        "    if Xspace is not None and Zspace is not None:\n",
        "        ax.contourf(*(Xspace.T), (Zspace.T[0]), cmap=cmap, levels=50)\n",
        "        ax.axis('off')\n",
        "    colors = ['red' if y else 'blue' for y in Y]\n",
        "    ax.scatter(*X.T, c=colors, edgecolors='black', s=50)\n",
        "    ax.set_title(title)\n",
        "    plt.tight_layout()\n",
        "    return ax\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTiw8lClkCeD"
      },
      "outputs": [],
      "source": [
        "def generate_spiral_dataset(num_per_class=500, validation_split=0.2, test_split=0.3, zero_var=1., one_var=1., shuffle=True, key=0):\n",
        "    \"\"\"Generate balanced, standardized 2d \"spiral\" binary classification dataset.\n",
        "    Code adapted from https://gist.github.com/45deg/e731d9e7f478de134def5668324c44c5\n",
        "    Args:\n",
        "        num_per_class (int, optional): Number of points to generate per class. Defaults to 250.\n",
        "        zero_val (float, optional): Noise variance for inputs withj label '0'. Defaults to 1.\n",
        "        one_val (float, optional): Noise variance for inputs withj label '1'. Defaults to 1.\n",
        "        shuffle (bool, optional): Flag to determine whether to return shuffled dataset. Defaults to True.\n",
        "        key (int, optional): Initial PRNG seed for jax.random. Defaults to 0.\n",
        "    Returns:\n",
        "        input: Generated input.\n",
        "        output: Generated binary output.\n",
        "    \"\"\"    \n",
        "    if isinstance(key, int):\n",
        "        key = jr.PRNGKey(key)\n",
        "    key1, key2, key3, key4 = jr.split(key, 4)\n",
        "\n",
        "    theta = jnp.sqrt(jr.uniform(key1, shape=(num_per_class,))) * 2*jnp.pi\n",
        "    r = 2*theta + jnp.pi\n",
        "    generate_data = lambda theta, r: jnp.array([jnp.cos(theta)*r, jnp.sin(theta)*r]).T\n",
        "\n",
        "    # Input data for output zero\n",
        "    zero_input = generate_data(theta, r) + zero_var * jr.normal(key2, shape=(num_per_class, 2))\n",
        "\n",
        "    # Input data for output one\n",
        "    one_input = generate_data(theta, -r) + one_var * jr.normal(key3, shape=(num_per_class, 2))\n",
        "\n",
        "    # Stack the inputs and standardize\n",
        "    input = jnp.concatenate([zero_input, one_input])\n",
        "    input = (input - input.mean(axis=0)) / input.std(axis=0)\n",
        "\n",
        "    # Generate binary output\n",
        "    output = jnp.concatenate([jnp.zeros(num_per_class), jnp.ones(num_per_class)])\n",
        "\n",
        "    if shuffle:\n",
        "        idx = jr.permutation(key4, jnp.arange(num_per_class * 2))\n",
        "        input, output = input[idx], output[idx]\n",
        "\n",
        "    val_index, test_index = num_per_class, int(1.4 * num_per_class)\n",
        "    X_train, X_val, X_test = input[:val_index], input[val_index:test_index], input[test_index:]\n",
        "    y_train, y_val, y_test = output[:val_index], output[val_index:test_index], output[test_index:]\n",
        "\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xyDopLBkHAL"
      },
      "outputs": [],
      "source": [
        "num_per_class=1000\n",
        "X_train, X_val, X_test, y_train, y_val, y_test = generate_spiral_dataset(num_per_class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgrllPVYkIKR"
      },
      "outputs": [],
      "source": [
        "print(X_train.shape)\n",
        "print(X_val.shape)\n",
        "print(X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7ea71elmhki"
      },
      "outputs": [],
      "source": [
        "init_vars = jnp.array([jnp.arange(9, 0, -1) * dec for dec in [1e-1, 1e-2, 1e-3]]).ravel()\n",
        "model_dim_grid = {'MLP1': [2, 1], \n",
        "                  'MLP2': [2, 100, 1], \n",
        "                  'MLP3': [2, 30, 30, 1], \n",
        "                  'MLP4': [2, 20, 20, 20, 1]}\n",
        "lrs = ms = jnp.array([1e-1, 5e-2, 1e-2, 5e-3, 1e-3, 5e-4, 1e-4, 1e-5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9HL4XuWkYMH"
      },
      "source": [
        "# 2. Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2iJPPqJDm4zL"
      },
      "outputs": [],
      "source": [
        "def reg_loss_fn(params, x, y, prior_dist, apply_fn, lamb):\n",
        "    neg_log_prior = -prior_dist.log_prob(params)\n",
        "    \n",
        "    y, y_hat = jnp.atleast_1d(y), apply_fn(params, x)\n",
        "    loss_value = optax.sigmoid_binary_cross_entropy(y_hat, y)\n",
        "    nll = loss_value.mean()\n",
        "    \n",
        "    return nll + lamb * neg_log_prior"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zmdez2U_m-ct"
      },
      "outputs": [],
      "source": [
        "def fit_optax(params, optimizer, input, output, loss_fn, num_epochs, return_history=False,\n",
        "              polyak_averaging=False, polyak_window_len=20):\n",
        "    opt_state = optimizer.init(params)\n",
        "\n",
        "    @jax.jit\n",
        "    def step(params, opt_state, x, y):\n",
        "        loss_value, grads = jax.value_and_grad(loss_fn)(params, x, y)\n",
        "        updates, opt_state = optimizer.update(grads, opt_state, params)\n",
        "        params = optax.apply_updates(params, updates)\n",
        "        return params, opt_state, loss_value\n",
        "    \n",
        "    if return_history:\n",
        "        params_history = []\n",
        "        if polyak_averaging:\n",
        "            polyak_window = deque()\n",
        "    \n",
        "    _, unflatten_fn = ravel_pytree(params)\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        for i, (x, y) in enumerate(zip(input, output)):\n",
        "            params, opt_state, loss_value = step(params, opt_state, x, y)\n",
        "            if return_history:\n",
        "                if polyak_averaging:\n",
        "                    flattened_params, _  = ravel_pytree(params)\n",
        "                    polyak_window.append(flattened_params)\n",
        "                    if len(polyak_window) == polyak_window_len + 1:\n",
        "                        polyak_window.popleft()\n",
        "                    params = unflatten_fn(jnp.array(polyak_window).mean(axis=0))\n",
        "                params_history.append(params)\n",
        "    \n",
        "    if return_history:\n",
        "        return jnp.array(params_history)\n",
        "    return params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1SyzhAdnAzy"
      },
      "outputs": [],
      "source": [
        "@partial(jit, static_argnums=(2, 5, 6))\n",
        "def evaluate_posterior(params, cov, apply_fn, X_test, y_test, \n",
        "                       posterior_predictive_estimation='map',\n",
        "                       cov_type='full', num_samples=50, key=0):\n",
        "    \"\"\" Evaluate negative log likelihood for given parameters and test set\n",
        "    \"\"\"\n",
        "    if isinstance(key, int):\n",
        "        key = jr.PRNGKey(key)\n",
        "\n",
        "    @jit\n",
        "    def evaluate_nll(params, X, y):\n",
        "        logits = apply_fn(params, X)\n",
        "        return optax.sigmoid_binary_cross_entropy(logits, y)\n",
        "\n",
        "    def evaluate_ece(params, X, y):\n",
        "        eps = 1e-3\n",
        "        sigmoid_fn = lambda w, x: jnp.clip(jax.nn.sigmoid(apply_fn(w, x)), eps, 1-eps) # Clip to prevent divergence\n",
        "        pred = vmap(sigmoid_fn, (None, 0))(params, X)\n",
        "        pred = jnp.concatenate((1-pred, pred), axis=1)\n",
        "        return tfp.stats.expected_calibration_error(20, logits=jnp.log(pred), labels_true=y.astype(int))\n",
        "    \n",
        "    @jit\n",
        "    def evaluate_linearized_nll(params_map, params, X, y):\n",
        "        logit_fn = lambda p: apply_fn(p, X)\n",
        "        H = _jacrev_2d(logit_fn, params_map)\n",
        "        logits = jnp.clip(logit_fn(params_map) + H @ (params - params_map), -10, 10)\n",
        "        return optax.sigmoid_binary_cross_entropy(logits, y)\n",
        "\n",
        "    ece = evaluate_ece(params, X_test, y_test)\n",
        "\n",
        "    if posterior_predictive_estimation == 'map':\n",
        "        nlls = vmap(evaluate_nll, (None, 0, 0))(params, X_test, y_test)\n",
        "        result = {'nll': nlls.mean(), 'ece': ece}\n",
        "        return result\n",
        "\n",
        "    if cov_type == 'full':\n",
        "        mvn = MVN(loc=params, covariance_matrix=cov)\n",
        "    elif cov_type == 'diagonal':\n",
        "        mvn = MVND(loc=params, scale_diag=cov)\n",
        "    else:\n",
        "        raise ValueError()\n",
        "    sampled_params = mvn.sample(seed=key, sample_shape=num_samples)\n",
        "    \n",
        "    if posterior_predictive_estimation == 'mc':\n",
        "        evaluate_average_nll = lambda p: vmap(evaluate_nll, (None, 0, 0))(p, X_test, y_test)\n",
        "    elif posterior_predictive_estimation == 'immer':\n",
        "        evaluate_average_nll = lambda p: vmap(evaluate_linearized_nll, (None, None, 0, 0))(params, p, X_test, y_test)\n",
        "    else:\n",
        "        raise ValueError()\n",
        "\n",
        "    nlls = vmap(evaluate_average_nll)(sampled_params)\n",
        "    result = {'nll': nlls.mean(), 'ece': ece}\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eShP4RcTnKig"
      },
      "outputs": [],
      "source": [
        "def apply_hyperparam(init_var, model_dim, train_len):\n",
        "    # Generate MLP model with specified dimensions\n",
        "    _, flat_params, _, apply_fn = get_mlp_flattened_params(model_dim)\n",
        "\n",
        "    # FCEKF parameters\n",
        "    state_dim = flat_params.size\n",
        "    eps = 1e-4\n",
        "    sigmoid_fn = lambda w, x: jnp.clip(jax.nn.sigmoid(apply_fn(w, x)), eps, 1-eps) # Clip to prevent divergence\n",
        "    fcekf_params = ParamsGGSSM(\n",
        "        initial_mean=flat_params,\n",
        "        initial_covariance=jnp.eye(state_dim) * init_var,\n",
        "        dynamics_function=lambda w, _: w,\n",
        "        dynamics_covariance = jnp.eye(state_dim) * 0.,\n",
        "        emission_mean_function = lambda w, x: sigmoid_fn(w, x),\n",
        "        emission_cov_function = lambda w, x: sigmoid_fn(w, x) * (1 - sigmoid_fn(w, x))\n",
        "    )\n",
        "\n",
        "    # DEKF parameters\n",
        "    dekf_params = DEKFParams(\n",
        "        initial_mean=flat_params,\n",
        "        initial_cov_diag=jnp.ones((state_dim,)) * init_var,\n",
        "        dynamics_cov_diag=jnp.ones((state_dim,)) * 0.,\n",
        "        emission_mean_function = lambda w, x: sigmoid_fn(w, x),\n",
        "        emission_cov_function = lambda w, x: sigmoid_fn(w, x) * (1 - sigmoid_fn(w, x))\n",
        "    )\n",
        "\n",
        "    \n",
        "    # Loss function\n",
        "    prior_dist = MVN(loc=jnp.zeros(flat_params.shape), covariance_matrix=jnp.eye(state_dim) * init_var)\n",
        "    loss_fn = partial(reg_loss_fn, prior_dist = prior_dist, apply_fn = apply_fn, lamb = 1/train_len)\n",
        "\n",
        "    return fcekf_params, dekf_params, flat_params, loss_fn, apply_fn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4sMGq_5nSCF"
      },
      "outputs": [],
      "source": [
        "def evaluate_sgd(learning_rate, momentum, model_dim, X_train, y_train, X_test, y_test, regularize=False, init_var=None):\n",
        "    sgd_optimizer = optax.sgd(learning_rate=learning_rate, momentum=momentum)\n",
        "    _, flat_params, _, apply_fn = get_mlp_flattened_params(model_dim)\n",
        "    if regularize:\n",
        "        prior_dist = MVN(loc=jnp.zeros(flat_params.shape), covariance_matrix=jnp.eye(flat_params.size) * init_var)\n",
        "        train_len = len(y_train)\n",
        "        loss_fn = partial(reg_loss_fn, prior_dist=prior_dist, apply_fn = apply_fn, lamb=1/train_len)\n",
        "    else:\n",
        "        loss_fn = partial(loss_optax, \n",
        "                          loss_fn = lambda y, \n",
        "                          yhat: -(y * jnp.log(yhat) + (1-y) * jnp.log(1 - yhat)), \n",
        "                          apply_fn = lambda w, x: jax.nn.sigmoid(apply_fn(w, x)))\n",
        "    sgd_post = fit_optax(flat_params, sgd_optimizer, X_train, y_train, loss_fn, num_epochs=1, return_history=True)\n",
        "\n",
        "    evaluate_sgd = lambda p: evaluate_posterior(p, None, apply_fn, X_test, y_test)['nll']\n",
        "    sgd_result = vmap(evaluate_sgd)(sgd_post)\n",
        "    \n",
        "    # Take mean of nll over the final 'avg_window' parameters\n",
        "    avg_window = int(len(X_test) / 4)\n",
        "    return sgd_result[-avg_window:].mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fy6XMYOkucfb"
      },
      "source": [
        "# 3. SGD Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txOW5hAQnahQ"
      },
      "outputs": [],
      "source": [
        "def grid_search_sgd(learning_rates, momentums, init_var, model_dims, X_train, y_train, X_test, y_test, regularize=False):\n",
        "    def _evaluate_sgd(mdim):\n",
        "        mdim = list(mdim)\n",
        "        _evaluate_sgd_mdim = lambda lr, m: evaluate_sgd(lr, m, mdim, X_train, y_train, X_test, y_test, regularize, init_var)\n",
        "        result = vmap(vmap(_evaluate_sgd_mdim, (None, 0)), (0, None))(learning_rates, momentums)\n",
        "        lr_indx, m_indx = jnp.unravel_index(result.argmin(), result.shape)\n",
        "        lr_opt, m_opt = learning_rates[lr_indx], momentums[m_indx]\n",
        "\n",
        "        return lr_opt, m_opt\n",
        "\n",
        "    model_dims = {key: jnp.array(val) for key, val in model_dims.items()}\n",
        "    result_sgd = tree_map(\n",
        "        lambda model_dim: _evaluate_sgd(model_dim),\n",
        "        model_dims\n",
        "    )\n",
        "\n",
        "    return result_sgd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wa6lXX_coLRk"
      },
      "outputs": [],
      "source": [
        "gridsearch_result = dict()\n",
        "for init_var in tqdm(init_vars):\n",
        "    tmp_result = grid_search_sgd(lrs, ms, init_var, model_dim_grid, X_train, y_train, X_val, y_val, regularize=True)\n",
        "    for key, val in tmp_result.items():\n",
        "        lr, m = float(val[0]), float(val[1])\n",
        "        if key not in gridsearch_result:\n",
        "            gridsearch_result[key] = dict()\n",
        "        gridsearch_result[key][f'{init_var:.3f}'] = (lr, m)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlUGFyCiun1I"
      },
      "source": [
        "# 4. Model Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrH3_a4vuo40"
      },
      "outputs": [],
      "source": [
        "def generate_model_comparison_table(init_var_grid, model_dim_grid, X_train, y_train, X_test, y_test, sgd_gridsearch_result):\n",
        "    fcekf_result, fdekf_result, vdekf_result, sgd_result = dict(), dict(), dict(), dict()\n",
        "    results = {'fcekf': fcekf_result, 'fdekf': fdekf_result, 'vdekf': vdekf_result, 'sgd': sgd_result}\n",
        "\n",
        "    for model_dim_type, model_dim in model_dim_grid.items():\n",
        "        pbar = tqdm(init_var_grid)\n",
        "        for init_var in pbar:\n",
        "            pbar.set_description(f'model_dim={model_dim_type}, init_var={init_var:.3f}')\n",
        "            curr_index = f'{model_dim_type}_{init_var}'\n",
        "            fcekf_params, dekf_params, flat_params, loss_fn, apply_fn = apply_hyperparam(init_var, model_dim, len(y_train))\n",
        "            fcekf_post = conditional_moments_gaussian_filter(fcekf_params, EKFIntegrals(), y_train, inputs=X_train)\n",
        "            fdekf_post = stationary_dynamics_fully_decoupled_conditional_moments_gaussian_filter(dekf_params, y_train, inputs=X_train)\n",
        "            vdekf_post = stationary_dynamics_variational_diagonal_extended_kalman_filter(dekf_params, y_train, inputs=X_train)\n",
        "            posts = {'fcekf': fcekf_post, 'fdekf': fdekf_post, 'vdekf': vdekf_post}\n",
        "            for post_type, post in posts.items():\n",
        "                if post_type == 'fcekf':\n",
        "                    evaluate_posterior_map = lambda p, c: evaluate_posterior(p, c, apply_fn, X_test, y_test, 'map')\n",
        "                    evaluate_posterior_mc = lambda p, c: evaluate_posterior(p, c, apply_fn, X_test, y_test, 'mc')\n",
        "                    evaluate_posterior_immer = lambda p, c: evaluate_posterior(p, c, apply_fn, X_test, y_test, 'immer')\n",
        "                else:\n",
        "                    evaluate_posterior_map = lambda p, c: evaluate_posterior(p, c, apply_fn, X_test, y_test, 'map', 'diagonal')\n",
        "                    evaluate_posterior_mc = lambda p, c: evaluate_posterior(p, c, apply_fn, X_test, y_test, 'mc', 'diagonal')\n",
        "                    evaluate_posterior_immer = lambda p, c: evaluate_posterior(p, c, apply_fn, X_test, y_test, 'immer', 'diagonal')\n",
        "                result_map = vmap(evaluate_posterior_map, (0, 0))(post.filtered_means, post.filtered_covariances)\n",
        "                result_mc = vmap(evaluate_posterior_mc, (0, 0))(post.filtered_means, post.filtered_covariances)\n",
        "                result_immer = vmap(evaluate_posterior_immer, (0, 0))(post.filtered_means, post.filtered_covariances)\n",
        "                result_by_type = {'map': result_map, 'mc': result_mc, 'immer': result_immer}\n",
        "                for est_type, result_nll in result_by_type.items():\n",
        "                    for eval_type in ['nll', 'ece']:\n",
        "                        results[post_type][curr_index+'_'+est_type+'_'+eval_type] = result_nll[eval_type]\n",
        "            \n",
        "            # SGD Optimizer\n",
        "            sgd_optimizer = optax.sgd(learning_rate=sgd_gridsearch_result[model_dim_type][f'{init_var:.3f}'][0],\n",
        "                                      momentum=sgd_gridsearch_result[model_dim_type][f'{init_var:.3f}'][1])\n",
        "\n",
        "            sgd_post = fit_optax(flat_params, sgd_optimizer, X_train, y_train, loss_fn, num_epochs=1, return_history=True)\n",
        "            evaluate_sgd = lambda p: evaluate_posterior(p, None, apply_fn, X_test, y_test, 'map')\n",
        "            sgd_result = vmap(evaluate_sgd)(sgd_post)\n",
        "            for result_type in ['map', 'mc', 'immer']:\n",
        "                for eval_type in ['nll', 'ece']:\n",
        "                    results['sgd'][curr_index+'_'+result_type+'_'+eval_type] = sgd_result[eval_type]\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4pIZoBkrvMmN"
      },
      "outputs": [],
      "source": [
        "result = generate_model_comparison_table(init_vars, model_dim_grid, X_train, y_train, X_test, y_test, gridsearch_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rUl4uzrwf76"
      },
      "outputs": [],
      "source": [
        "# Store resulting NLLs and ECEs as .csv files\n",
        "for model_type in ['fcekf', 'fdekf', 'vdekf', 'sgd']:\n",
        "    df = pd.DataFrame.from_dict(result[model_type], orient='index')\n",
        "    filepath = Path(Path.cwd(), f'nll_results_{model_type}.csv')\n",
        "    filepath.parent.mkdir(parents=True, exist_ok=True)\n",
        "    df.to_csv(filepath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mx7B73Iuw0nN"
      },
      "outputs": [],
      "source": [
        "fcekf_df = pd.read_csv(Path(Path.cwd(), 'nll_results_fcekf.csv'), index_col=0)\n",
        "fdekf_df = pd.read_csv(Path(Path.cwd(), 'nll_results_fdekf.csv'), index_col=0)\n",
        "vdekf_df = pd.read_csv(Path(Path.cwd(), 'nll_results_vdekf.csv'), index_col=0)\n",
        "sgd_df = pd.read_csv(Path(Path.cwd(), 'nll_results_sgd.csv'), index_col=0)\n",
        "dfs = {'fcekf': fcekf_df, 'fdekf': fdekf_df, 'vdekf': vdekf_df, 'sgd': sgd_df}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlBGAv0Kw1Gb"
      },
      "outputs": [],
      "source": [
        "train_steps = jnp.arange(len(X_train))\n",
        "\n",
        "predictive_dict = {'map': 'plugin', 'mc': 'MC', 'immer': 'linearized'}\n",
        "\n",
        "for init_var in init_vars:\n",
        "    for model_dim_type, model_dim in model_dim_grid.items():\n",
        "        fig, ax = plt.subplots()\n",
        "        for predictive_type in ['map', 'mc', 'immer']:\n",
        "            ax.plot(train_steps, fcekf_df.loc[f'{model_dim_type}_{init_var}_{predictive_type}_nll'], label=f'FCEKF-{predictive_dict[predictive_type]}')\n",
        "            ax.plot(train_steps, fdekf_df.loc[f'{model_dim_type}_{init_var}_{predictive_type}_nll'], label=f'FDEKF-{predictive_dict[predictive_type]}')\n",
        "            ax.plot(train_steps, vdekf_df.loc[f'{model_dim_type}_{init_var}_{predictive_type}_nll'], label=f'VDEKF-{predictive_dict[predictive_type]}')\n",
        "        ax.plot(train_steps, sgd_df.loc[f'{model_dim_type}_{init_var}_map_nll'], label='SGD')\n",
        "        ax.set_title(f'NLL comparison for Initial Var={init_var:.3f}, Model Dim=[{\",\".join(map(str, model_dim))}]')\n",
        "        ax.legend();\n",
        "        fig.savefig(Path(Path.cwd(), 'nll_all_models', f'{model_dim_type}_{init_var:.3f}.png'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3Qcac28LBI2"
      },
      "outputs": [],
      "source": [
        "train_steps = jnp.arange(len(X_train))\n",
        "\n",
        "predictive_dict = {'map': 'plugin', 'mc': 'MC', 'immer': 'linearized'}\n",
        "\n",
        "for init_var in init_vars:\n",
        "    for model_dim_type, model_dim in model_dim_grid.items():\n",
        "        fig, ax = plt.subplots()\n",
        "        for predictive_type in ['map', 'immer']:\n",
        "            ax.plot(train_steps, fcekf_df.loc[f'{model_dim_type}_{init_var}_{predictive_type}_nll'], label=f'FCEKF-{predictive_dict[predictive_type]}')\n",
        "            ax.plot(train_steps, fdekf_df.loc[f'{model_dim_type}_{init_var}_{predictive_type}_nll'], label=f'FDEKF-{predictive_dict[predictive_type]}')\n",
        "            ax.plot(train_steps, vdekf_df.loc[f'{model_dim_type}_{init_var}_{predictive_type}_nll'], label=f'VDEKF-{predictive_dict[predictive_type]}')\n",
        "        ax.plot(train_steps, sgd_df.loc[f'{model_dim_type}_{init_var}_map_nll'], label='SGD')\n",
        "        ax.set_title(f'NLL comparison for Initial Var.={init_var:.3f}, Model Dim. =[{\",\".join(map(str, model_dim))}]')\n",
        "        ax.legend();\n",
        "        fig.savefig(Path(Path.cwd(), 'nll_plugin_linearized', f'{model_dim_type}_{init_var:.3f}.png'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7YkHkVQLX_V"
      },
      "outputs": [],
      "source": [
        "train_steps = jnp.arange(len(X_train))\n",
        "\n",
        "predictive_dict = {'map': 'plugin', 'mc': 'MC', 'immer': 'linearized'}\n",
        "\n",
        "for init_var in init_vars:\n",
        "    for model_dim_type, model_dim in model_dim_grid.items():\n",
        "        fig, ax = plt.subplots()\n",
        "        for predictive_type in ['map', 'immer']:\n",
        "            ax.plot(train_steps, fcekf_df.loc[f'{model_dim_type}_{init_var}_{predictive_type}_nll'], label=f'FCEKF-{predictive_dict[predictive_type]}')\n",
        "            ax.plot(train_steps, fdekf_df.loc[f'{model_dim_type}_{init_var}_{predictive_type}_nll'], label=f'FDEKF-{predictive_dict[predictive_type]}')\n",
        "            ax.plot(train_steps, vdekf_df.loc[f'{model_dim_type}_{init_var}_{predictive_type}_nll'], label=f'VDEKF-{predictive_dict[predictive_type]}')\n",
        "        # ax.plot(train_steps, sgd_df.loc[f'{model_dim_type}_{init_var}_map_nll'], label='SGD')\n",
        "        ax.set_title(f'NLL comparison for Initial Var.={init_var:.3f}, Model Dim. =[{\",\".join(map(str, model_dim))}]')\n",
        "        ax.legend();\n",
        "        fig.savefig(Path(Path.cwd(), 'nll_ekfs', f'{model_dim_type}_{init_var:.3f}.png'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VJpBiDL1G3s"
      },
      "outputs": [],
      "source": [
        "train_steps = jnp.arange(len(X_train))\n",
        "\n",
        "for init_var in init_vars:\n",
        "    for model_dim_type, model_dim in model_dim_grid.items():\n",
        "        fig, ax = plt.subplots()\n",
        "        for predictive_type in ['map', 'immer']:\n",
        "            ax.plot(train_steps, fcekf_df.loc[f'{model_dim_type}_{init_var}_{predictive_type}_ece'], label=f'fcekf_{predictive_type}')\n",
        "            ax.plot(train_steps, fdekf_df.loc[f'{model_dim_type}_{init_var}_{predictive_type}_ece'], label=f'fdekf_{predictive_type}')\n",
        "            ax.plot(train_steps, vdekf_df.loc[f'{model_dim_type}_{init_var}_{predictive_type}_ece'], label=f'vdekf_{predictive_type}')\n",
        "        ax.plot(train_steps, sgd_df.loc[f'{model_dim_type}_{init_var}_map_ece'], label='sgd')\n",
        "        ax.set_title(f'ECE comparison for init_var={init_var:.3f}, model_dim=[{\",\".join(map(str, model_dim))}]')\n",
        "        ax.legend();\n",
        "        fig.savefig(Path(Path.cwd(), 'ece_all_models', f'{model_dim_type}_{init_var:.3f}.png'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbGBfDxgPt9t"
      },
      "source": [
        "# 5. Heat Maps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGsvcJ7LQr_L"
      },
      "outputs": [],
      "source": [
        "input_grid = generate_input_grid(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wZiTDVHMhAVK"
      },
      "outputs": [],
      "source": [
        "selected_mlps = ['MLP2', 'MLP3', 'MLP4']\n",
        "selected_init_vars = [0.1, 0.01, 0.001]\n",
        "X, y = X_train, y_train\n",
        "for mlp in selected_mlps:\n",
        "    model_dim = model_dim_grid[mlp]\n",
        "    for init_var in selected_init_vars:\n",
        "        curr_init_var = f'{init_var:.3f}'\n",
        "        fcekf_params, dekf_params, flat_params, loss_fn, apply_fn = apply_hyperparam(init_var, model_dim, len(X))\n",
        "        # EKF Models\n",
        "        fcekf_weight = conditional_moments_gaussian_filter(fcekf_params, EKFIntegrals(), y, inputs=X).filtered_means[-1]\n",
        "        fcekf_cov = conditional_moments_gaussian_filter(fcekf_params, EKFIntegrals(), y, inputs=X).filtered_covariances[-1]\n",
        "        fdekf_weight = stationary_dynamics_fully_decoupled_conditional_moments_gaussian_filter(dekf_params, y, inputs=X).filtered_means[-1]\n",
        "        fdekf_cov = stationary_dynamics_fully_decoupled_conditional_moments_gaussian_filter(dekf_params, y, inputs=X).filtered_covariances[-1]\n",
        "        vdekf_weight = stationary_dynamics_variational_diagonal_extended_kalman_filter(dekf_params, y, inputs=X).filtered_means[-1]\n",
        "        vdekf_cov = stationary_dynamics_variational_diagonal_extended_kalman_filter(dekf_params, y, inputs=X).filtered_covariances[-1]\n",
        "        \n",
        "        #SGD\n",
        "        lr, m = gridsearch_result[mlp][curr_init_var]\n",
        "        sgd_optimizer = optax.sgd(learning_rate=lr, momentum=m)\n",
        "        sgd_weight = fit_optax(flat_params, sgd_optimizer, X, y, loss_fn, 1, True)[-1]\n",
        "        eps = 1e-3\n",
        "        sigmoid_fn = lambda w, x: jnp.clip(jax.nn.sigmoid(apply_fn(w, x)), eps, 1-eps)\n",
        "        \n",
        "        # Plot\n",
        "        ekf_params = {'fcekf': (fcekf_weight, fcekf_cov), \n",
        "                      'fdekf': (fdekf_weight, fdekf_cov),\n",
        "                      'vdekf': (vdekf_weight, vdekf_cov)}\n",
        "        \n",
        "        for ekf_type, params in ekf_params.items():\n",
        "            weight, cov = params\n",
        "            Z_plugin = posterior_predictive_grid(input_grid, weight, sigmoid_fn, binary=False)\n",
        "            Z_mc = posterior_predictive_grid_ekf(input_grid, weight, cov, sigmoid_fn, binary=False, ekf_type=ekf_type)\n",
        "            Z_linearized = posterior_predictive_grid_ekf(input_grid, weight, cov, sigmoid_fn, binary=False, ekf_type=ekf_type, post_pred_type='immer')\n",
        "            for Z_type, Z in {'Plugin': Z_plugin, 'MC': Z_mc, 'Linearized': Z_linearized}.items():\n",
        "                fig, ax = plt.subplots(figsize=(6, 5))\n",
        "                title=f'{ekf_type.upper()} {Z_type}: Initial Var={curr_init_var}, Model Dim=[{\",\".join(map(str, model_dim))}]'\n",
        "                plot_posterior_predictive(ax, X, y, title, input_grid, Z);\n",
        "                fig.savefig(f'{ekf_type}_{Z_type}_{mlp}_{init_var:.3f}.png', bbox_inches = 'tight')\n",
        "\n",
        "        Z_sgd = posterior_predictive_grid(input_grid, sgd_weight, sigmoid_fn, binary=False)\n",
        "        fig, ax = plt.subplots(figsize=(6, 5))\n",
        "        title=f'SGD: Initial Var={curr_init_var}, Model Dim=[{\",\".join(map(str, model_dim))}]'\n",
        "        plot_posterior_predictive(ax, X, y, title, input_grid, Z_sgd);\n",
        "        fig.savefig(f'sgd_{mlp}_{init_var:.3f}.png', bbox_inches = 'tight')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCBJIFT9htMg"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "fy6XMYOkucfb"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}